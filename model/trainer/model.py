# Importing libraries
import google.cloud.bigquery as bq
import tensorflow as tf
import shutil

print("Tensorflow version: " + tf.__version__)
tf.logging.set_verbosity(tf.logging.INFO)


# Using Bigquery to get column names(As in train dataset column names are not present)
client = bq.Client.from_service_account_json('/home/soumyadip/Downloads/ProjectNaMoRaGaTwitterBattle-5bd944d1b0f7.json')
query1 = """SELECT * FROM mmp.train LIMIT 1"""
datapreview = client.query(query1).to_dataframe()

# #### Total number of records = 8,914,868 (each machine has only one record)
# #### Total number of predictors = 81(Excluding machine identifier)
# #### Machine Identifier = "MachineIdentifier" 
# #### Predicted varibale = "HasDetections"
# #### Predicted varibale distribution = 4458013 (negative) / 4456855 (positive)

# Columns present in csv (83)
CSV_COLUMNS = list(datapreview.columns)[1:]  # Predictor variables

# Columns needed for training (82)
COLUMN_NAMES = []
for i in range(1, 83):
    COLUMN_NAMES.append(str(list(datapreview.columns)[i]))  # Converting unicode to string

# Predicted variable
LABEL_COLUMN = 'HasDetections'

# Defining default values and datatype for columns
DEFAULTS = [["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], [0],
            ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"],

            ["NotAvailable"], [0], ["NotAvailable"], ["NotAvailable"], [0],
            ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"],

            ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"],
            ["NotAvailable"], [0], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"],

            ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"],
            ["NotAvailable"], ["NotAvailable"], [0], ["NotAvailable"], ["NotAvailable"],

            ["NotAvailable"], tf.constant([0], dtype=tf.float64), ["NotAvailable"], [0], [0],
            [0], ["NotAvailable"], [0.0], [0], [0],

            ["NotAvailable"], ["NotAvailable"], tf.constant([0], dtype=tf.float64), ["NotAvailable"], ["NotAvailable"],
            ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"],

            ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], [0],
            ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"], ["NotAvailable"],

            ["NotAvailable"], ["NotAvailable"], [0], [0], ["NotAvailable"],
            ["NotAvailable"], [0], [0], ["NotAvailable"], ["NotAvailable"],

            ["NotAvailable"], [0]
            ]

# Creating feature column for inputs
fc = tf.feature_column

INPUT_COLUMNS = []  # Initilizing empty column

# Applying different types of feature columns transformation
# 1
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="ProductName", hash_bucket_size=6, dtype=tf.string), dimension=2))
# 2
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="EngineVersion", hash_bucket_size=70, dtype=tf.string), dimension=3))
# 3
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="AppVersion", hash_bucket_size=110, dtype=tf.string), dimension=4))
# 4
INPUT_COLUMNS.append(fc.embedding_column(
    categorical_column=fc.categorical_column_with_hash_bucket(
        key="AvSigVersion", hash_bucket_size=100, dtype=tf.string), dimension=4))
# 5                    
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_identity(
        key="IsBeta", num_buckets=2)))
# 6                    
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="RtpStateBitfield", hash_bucket_size=8, dtype=tf.string), dimension=2))
# 7                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="IsSxsPassiveMode", hash_bucket_size=3, dtype=tf.string)))
# 8                   
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="DefaultBrowsersIdentifier", hash_bucket_size=100, dtype=tf.string), dimension=4))
# 9                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="AVProductStatesIdentifier", hash_bucket_size=100, dtype=tf.string), dimension=4))
# 10                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="AVProductsInstalled", hash_bucket_size=9, dtype=tf.string), dimension=3))
# 11
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="AVProductsEnabled", hash_bucket_size=7, dtype=tf.string), dimension=2))
# 12                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_identity(
        key="HasTpm", num_buckets=2)))
# 13                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="CountryIdentifier", hash_bucket_size=222, dtype=tf.string), dimension=4))
# 14                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="CityIdentifier", hash_bucket_size=100, dtype=tf.string), dimension=4))
# 15                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="OrganizationIdentifier", hash_bucket_size=50, dtype=tf.int32), dimension=3))
# 16                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="GeoNameIdentifier", hash_bucket_size=293, dtype=tf.string), dimension=5))
# 17                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="LocaleEnglishNameIdentifier", hash_bucket_size=276, dtype=tf.string), dimension=5))
# 18                    
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Platform", hash_bucket_size=4, dtype=tf.string)))
# 19                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Processor", hash_bucket_size=3, dtype=tf.string)))
# 20                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="OsVer", hash_bucket_size=58, dtype=tf.string), dimension=3))
# 21                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="OsBuild", hash_bucket_size=76, dtype=tf.string), dimension=3))
# 22                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="OsSuite", hash_bucket_size=14, dtype=tf.string), dimension=2))
# 23                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="OsPlatformSubRelease", hash_bucket_size=9, dtype=tf.string), dimension=2))
# 24                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="OsBuildLab", hash_bucket_size=100, dtype=tf.string), dimension=4))
# 25                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="SkuEdition", hash_bucket_size=8, dtype=tf.string), dimension=2))
# 26                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="IsProtected", hash_bucket_size=3, dtype=tf.string)))
# 27                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_identity(key="AutoSampleOptIn", num_buckets=2)))
# 28                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="PuaMode", hash_bucket_size=3, dtype=tf.string)))
# 29                    
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="SMode", hash_bucket_size=3, dtype=tf.string)))
# 30                    
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="IeVerIdentifier", hash_bucket_size=304, dtype=tf.string), dimension=5))
# 31
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="SmartScreen", hash_bucket_size=22, dtype=tf.string), dimension=3))
# 32                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Firewall", hash_bucket_size=3, dtype=tf.string)))
# 33                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="UacLuaenable", hash_bucket_size=12, dtype=tf.string), dimension=2))
# 34                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_MDC2FormFactor", hash_bucket_size=13, dtype=tf.string), dimension=2))
# 35                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_DeviceFamily", hash_bucket_size=3, dtype=tf.string)))
# 36                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OEMNameIdentifier", hash_bucket_size=100, dtype=tf.string), dimension=4))
# 37                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OEMModelIdentifier", hash_bucket_size=100, dtype=tf.string), dimension=4))
# 38                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.bucketized_column(
        source_column=fc.numeric_column(
            key="Census_ProcessorCoreCount",
            normalizer_fn=lambda x: tf.subtract(x, tf.reduce_mean(x))),
        boundaries=[2, 4, 8, 16, 32]),
    dimension=2))
# 39                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_ProcessorManufacturerIdentifier", hash_bucket_size=8, dtype=tf.string), dimension=2))
# 40                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_ProcessorModelIdentifier", hash_bucket_size=100, dtype=tf.string), dimension=4))
# 41
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_ProcessorClass", hash_bucket_size=4, dtype=tf.string), dimension=2))
# 42                     
INPUT_COLUMNS.append(fc.numeric_column(key="Census_PrimaryDiskTotalCapacity",
                                       normalizer_fn=lambda x: tf.subtract(x, tf.reduce_mean(x))))
# 43                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_PrimaryDiskTypeName", hash_bucket_size=5, dtype=tf.string), dimension=2))
# 44                     
INPUT_COLUMNS.append(fc.numeric_column(key="Census_SystemVolumeTotalCapacity",
                                       normalizer_fn=lambda x: tf.subtract(x, tf.reduce_mean(x))))
# 45                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_identity(
        key="Census_HasOpticalDiskDrive", num_buckets=2)))
# 46                     
INPUT_COLUMNS.append(fc.numeric_column(key="Census_TotalPhysicalRAM",
                                       normalizer_fn=lambda x: tf.subtract(x, tf.reduce_mean(x))))
# 47                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_ChassisTypeName", hash_bucket_size=53, dtype=tf.string), dimension=3))
# 48                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.bucketized_column(
        source_column=fc.numeric_column(
            key="Census_InternalPrimaryDiagonalDisplaySizeInInches", dtype=tf.float32,
            normalizer_fn=lambda x: tf.subtract(x, tf.reduce_mean(x))),
        boundaries=[10, 20, 30, 40, 50, 60]), dimension=2))
# 49                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.bucketized_column(
        source_column=fc.numeric_column(
            key="Census_InternalPrimaryDisplayResolutionHorizontal",
            normalizer_fn=lambda x: tf.subtract(x, tf.reduce_mean(x))),
        boundaries=[500, 1000, 1500, 2000]), dimension=2))
# 50                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.bucketized_column(
        source_column=fc.numeric_column(
            key="Census_InternalPrimaryDisplayResolutionVertical",
            normalizer_fn=lambda x: tf.subtract(x, tf.reduce_mean(x))),
        boundaries=[500, 1000, 1500, 2000]), dimension=2))
# 51
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_PowerPlatformRoleName", hash_bucket_size=11, dtype=tf.string), dimension=2))
# 52                    
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_InternalBatteryType", hash_bucket_size=80, dtype=tf.string), dimension=3))
# 53                   
INPUT_COLUMNS.append(fc.embedding_column(
    fc.bucketized_column(
        source_column=fc.numeric_column(
            key="Census_InternalBatteryNumberOfCharges",
            dtype=tf.int64,
            normalizer_fn=lambda x: tf.subtract(x, tf.reduce_mean(x))),
        boundaries=[1, 100, 1000, 10000, 100000, 1000000]),
    dimension=2))
# 54                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OSVersion", hash_bucket_size=469, dtype=tf.string), dimension=5))
# 55                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OSArchitecture", hash_bucket_size=3, dtype=tf.string)))
# 56                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OSBranch", hash_bucket_size=32, dtype=tf.string), dimension=3))
# 57                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OSBuildNumber", hash_bucket_size=165, dtype=tf.string), dimension=4))
# 58                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OSBuildRevision", hash_bucket_size=285, dtype=tf.string), dimension=5))
# 59                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OSEdition", hash_bucket_size=33, dtype=tf.string), dimension=3))
# 60                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OSSkuName", hash_bucket_size=30, dtype=tf.string), dimension=3))
# 61
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OSInstallTypeName", hash_bucket_size=9, dtype=tf.string), dimension=2))
# 62                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OSInstallLanguageIdentifier", hash_bucket_size=40, dtype=tf.string), dimension=3))
# 63                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OSUILocaleIdentifier", hash_bucket_size=147, dtype=tf.string), dimension=4))
# 64                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_OSWUAutoUpdateOptionsName", hash_bucket_size=6, dtype=tf.string), dimension=2))
# 65
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_identity(
        key="Census_IsPortableOperatingSystem", num_buckets=2)))
# 66
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_GenuineStateName", hash_bucket_size=5, dtype=tf.string), dimension=2))
# 67
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_ActivationChannel", hash_bucket_size=6, dtype=tf.string), dimension=2))
# 68
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_IsFlightingInternal", hash_bucket_size=3, dtype=tf.string)))
# 69
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_IsFlightsDisabled", hash_bucket_size=3, dtype=tf.string)))
# 70
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_FlightRing", hash_bucket_size=10, dtype=tf.string), dimension=2))
# 71
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_ThresholdOptIn", hash_bucket_size=3, dtype=tf.string)))
# 72                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_FirmwareManufacturerIdentifier", hash_bucket_size=100, dtype=tf.string), dimension=4))
# 73                     
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_FirmwareVersionIdentifier", hash_bucket_size=100, dtype=tf.int32), dimension=4))
# 74                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_identity(
        key="Census_IsSecureBootEnabled", num_buckets=2)))
# 75                    
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_IsWIMBootEnabled", hash_bucket_size=3, dtype=tf.string)))
# 76                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_IsVirtualDevice", hash_bucket_size=3, dtype=tf.string)))
# 77                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_identity(
        key="Census_IsTouchEnabled", num_buckets=2)))
# 78                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_identity(
        key="Census_IsPenCapable", num_buckets=2)))
# 79                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Census_IsAlwaysOnAlwaysConnectedCapable", hash_bucket_size=3, dtype=tf.string)))
# 80                     
INPUT_COLUMNS.append(fc.indicator_column(
    fc.categorical_column_with_hash_bucket(
        key="Wdft_IsGamer", hash_bucket_size=3, dtype=tf.string)))
# 81
INPUT_COLUMNS.append(fc.embedding_column(
    fc.categorical_column_with_hash_bucket(
        key="Wdft_RegionIdentifier", hash_bucket_size=16, dtype=tf.string), dimension=2))


# Input functions for training and evalution
def read_dataset(filename, mode, batch_size=128):
    def _input_fn():
        def decode_csv(value_column):
            columns = tf.io.decode_csv(value_column, record_defaults=DEFAULTS, select_cols=list(range(1, 83)))
            features = dict(zip(COLUMN_NAMES, columns))
            label = features.pop(LABEL_COLUMN)
            print(features)
            return features, label

        # Create list of files that match pattern
        file_list = tf.gfile.Glob(filename)

        dataset = tf.data.TextLineDataset(filenames=file_list).skip(1).map(decode_csv)

        if mode == tf.estimator.ModeKeys.TRAIN:
            num_epochs = None
            dataset = dataset.shuffle(buffer_size=10 * batch_size)
        else:
            num_epochs = 1

        dataset = dataset.repeat(num_epochs).batch(batch_size)
        return dataset.make_one_shot_iterator().get_next()

    return _input_fn


# Main train and evaluation loop
def train_and_evaluate(args):
    classifier = tf.estimator.DNNClassifier(
        hidden_units=args['hidden_units'],
        feature_columns=INPUT_COLUMNS,
        model_dir=args['output_dir'],
        n_classes=2,

        optimizer=lambda: tf.train.AdamOptimizer(
            learning_rate=0.01
            #                     learning_rate=tf.train.exponential_decay(
            #                     learning_rate=0.1,
            #                     global_step=tf.train.get_global_step(),
            #                     decay_steps=10000,
            #                     decay_rate=0.96)
        ),

        activation_fn=tf.nn.relu,
        dropout=None,
        input_layer_partitioner=None,
        config=None,
        warm_start_from=None,
        batch_norm=False
    )

    train_spec = tf.estimator.TrainSpec(
        input_fn=read_dataset(filename=args['train_data_paths'],
                              batch_size=args['train_batch_size'],
                              mode=tf.estimator.ModeKeys.TRAIN),
        max_steps=args['train_steps'])

    eval_spec = tf.estimator.EvalSpec(
        input_fn=read_dataset(filename=args['eval_data_paths'],
                              batch_size=1000,
                              mode=tf.estimator.ModeKeys.EVAL),
        steps=100,
        start_delay_secs=args['eval_delay_secs'],
        throttle_secs=args['min_eval_frequency'],
        exporters=None)

    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)
